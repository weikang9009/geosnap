{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geosnap provides a set of tools for collecting data and constructing space-time datasets, identifying local neighborhoods or prototypical neighborhood types, modeling neighborhood change over time, and visualizing data at each step of the process.\n",
    "\n",
    "geosnap works with data from anywhere in the world, but comes batteries-included with three decades of national US Census data, including boundaries for metropolitan statistical areas, states, counties, and tracts, and over 100 commonly used demographic and socioeconomic variables at the census-tract level. All of these data are stored as geopandas geodataframes in efficient [apache parquet](https://parquet.apache.org/) files and distributed through [quilt](https://quiltdata.com/). \n",
    "\n",
    "These data are available when you first import geosnap by streaming from our [quilt bucket](https://spatialucr.quiltdata.com/b/spatial-ucr) into memory. That can be useful if you dont need US data or if you just want to kick the tires, but it also means you need an internet connection to work with census data, and things may slow down depending on your network performance. For that reason, you can also use the `store_census` function to cache the data on your local machine for faster querying. This will only take around 400mb of disk space, speed up data operations, and remove the need for an internet connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using built-in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access geosnap's built-in data from the `datasets` module. It contains a variable codebook as well as state, county, and MSA boundaries, in addition to boundaries and social data for three decades of census tracts. If you have stored an existing longitudinal database such as LTDB or the Geolytics Neighborhood Change Database, it will be available in `datasets` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geosnap import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['blocks_2000',\n 'blocks_2010',\n 'codebook',\n 'counties',\n 'ltdb',\n 'msa_definitions',\n 'msas',\n 'ncdb',\n 'states',\n 'tracts_1990',\n 'tracts_2000',\n 'tracts_2010']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything in `datasets` is a pandas (or geopandas) geo/dataframe. To access any of the data inside, just call the appropriate attribute/method (most datasets are methods). For example, to accesss the codebook which outlines each variable in the data store, incuding its name, description, the original census sources/variable names and the formula used to calculate it, you simply call `datasets.codebook()`. We support the same variable set the Longitudinal Tract Database (LTDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>variable</th>\n      <th>label</th>\n      <th>formula</th>\n      <th>ltdb</th>\n      <th>ncdb</th>\n      <th>census_1990_form</th>\n      <th>census_1990_table_column</th>\n      <th>census_2000_form</th>\n      <th>census_2000_table_column</th>\n      <th>acs</th>\n      <th>category</th>\n      <th>notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>189</th>\n      <td>p_poverty_rate_black</td>\n      <td>percentage of blacks in poverty</td>\n      <td>p_poverty_rate_black=n_poverty_black / n_pover...</td>\n      <td>pbpov</td>\n      <td>BLKPR</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Socioeconomic Status</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>p_poverty_rate_hispanic</td>\n      <td>percentage of Hispanics in poverty</td>\n      <td>p_poverty_rate_hispanic=n_poverty_hispanic / n...</td>\n      <td>phpov</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Socioeconomic Status</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>p_poverty_rate_native</td>\n      <td>percentage of Native Americans in poverty</td>\n      <td>p_poverty_rate_native=n_poverty_native / n_pov...</td>\n      <td>pnapov</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Socioeconomic Status</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>p_poverty_rate_asian</td>\n      <td>percentage of Asian and Pacific Islanders in p...</td>\n      <td>p_poverty_rate_asian=n_poverty_asian / n_pover...</td>\n      <td>papov</td>\n      <td>RASPR</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Socioeconomic Status</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>n_total_pop</td>\n      <td>total population</td>\n      <td>NaN</td>\n      <td>pop</td>\n      <td>TRCTPOP</td>\n      <td>SF1</td>\n      <td>P0010001</td>\n      <td>SF1</td>\n      <td>P001001</td>\n      <td>B01001_001E</td>\n      <td>total population</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    variable  \\\n189     p_poverty_rate_black   \n190  p_poverty_rate_hispanic   \n191    p_poverty_rate_native   \n192     p_poverty_rate_asian   \n193              n_total_pop   \n\n                                                 label  \\\n189                    percentage of blacks in poverty   \n190                 percentage of Hispanics in poverty   \n191          percentage of Native Americans in poverty   \n192  percentage of Asian and Pacific Islanders in p...   \n193                                   total population   \n\n                                               formula    ltdb     ncdb  \\\n189  p_poverty_rate_black=n_poverty_black / n_pover...   pbpov    BLKPR   \n190  p_poverty_rate_hispanic=n_poverty_hispanic / n...   phpov      NaN   \n191  p_poverty_rate_native=n_poverty_native / n_pov...  pnapov      NaN   \n192  p_poverty_rate_asian=n_poverty_asian / n_pover...   papov    RASPR   \n193                                                NaN     pop  TRCTPOP   \n\n    census_1990_form census_1990_table_column census_2000_form  \\\n189              NaN                      NaN              NaN   \n190              NaN                      NaN              NaN   \n191              NaN                      NaN              NaN   \n192              NaN                      NaN              NaN   \n193              SF1                 P0010001              SF1   \n\n    census_2000_table_column          acs              category notes  \n189                      NaN          NaN  Socioeconomic Status   NaN  \n190                      NaN          NaN  Socioeconomic Status   NaN  \n191                      NaN          NaN  Socioeconomic Status   NaN  \n192                      NaN          NaN  Socioeconomic Status   NaN  \n193                  P001001  B01001_001E      total population   NaN  "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.codebook().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take a look at the dataframes themselves or plot them as quick choropleth maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>geoid</th>\n      <th>median_contract_rent</th>\n      <th>median_home_value</th>\n      <th>median_household_income</th>\n      <th>median_income_asianhh</th>\n      <th>median_income_blackhh</th>\n      <th>median_income_hispanichh</th>\n      <th>median_income_whitehh</th>\n      <th>n_age_5_older</th>\n      <th>n_asian_age_distribution</th>\n      <th>...</th>\n      <th>p_vacant_housing_units</th>\n      <th>p_veterans</th>\n      <th>p_vietnamese_persons</th>\n      <th>p_white_over_60</th>\n      <th>p_white_over_65</th>\n      <th>p_white_under_15</th>\n      <th>p_widowed_divorced</th>\n      <th>per_capita_income</th>\n      <th>year</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25009266400</td>\n      <td>662</td>\n      <td>172400</td>\n      <td>53314</td>\n      <td>48750</td>\n      <td>22500</td>\n      <td>0</td>\n      <td>53808</td>\n      <td>3065</td>\n      <td>7</td>\n      <td>...</td>\n      <td>3.74</td>\n      <td>9.84</td>\n      <td>0.03</td>\n      <td>14.59</td>\n      <td>11.04</td>\n      <td>21.40</td>\n      <td>17.61</td>\n      <td>24288</td>\n      <td>2000</td>\n      <td>POLYGON ((-70.91489900000001 42.886589, -70.90...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25009267102</td>\n      <td>653</td>\n      <td>169200</td>\n      <td>50739</td>\n      <td>46250</td>\n      <td>0</td>\n      <td>0</td>\n      <td>51264</td>\n      <td>4311</td>\n      <td>23</td>\n      <td>...</td>\n      <td>3.22</td>\n      <td>13.42</td>\n      <td>0.06</td>\n      <td>13.77</td>\n      <td>9.67</td>\n      <td>21.02</td>\n      <td>18.91</td>\n      <td>20946</td>\n      <td>2000</td>\n      <td>POLYGON ((-70.91489900000001 42.886589, -70.91...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>25009266200</td>\n      <td>662</td>\n      <td>163200</td>\n      <td>49315</td>\n      <td>90457</td>\n      <td>101277</td>\n      <td>26250</td>\n      <td>48150</td>\n      <td>5131</td>\n      <td>37</td>\n      <td>...</td>\n      <td>3.30</td>\n      <td>9.10</td>\n      <td>0.07</td>\n      <td>13.44</td>\n      <td>10.63</td>\n      <td>22.38</td>\n      <td>20.74</td>\n      <td>21817</td>\n      <td>2000</td>\n      <td>POLYGON ((-70.93079899999999 42.884589, -70.92...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25009267101</td>\n      <td>624</td>\n      <td>179200</td>\n      <td>45625</td>\n      <td>0</td>\n      <td>54545</td>\n      <td>38750</td>\n      <td>44750</td>\n      <td>3011</td>\n      <td>11</td>\n      <td>...</td>\n      <td>42.08</td>\n      <td>9.96</td>\n      <td>0.00</td>\n      <td>19.89</td>\n      <td>14.78</td>\n      <td>16.97</td>\n      <td>27.61</td>\n      <td>22578</td>\n      <td>2000</td>\n      <td>POLYGON ((-70.8246893731782 42.87092164133018,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25009266100</td>\n      <td>569</td>\n      <td>215200</td>\n      <td>60677</td>\n      <td>48750</td>\n      <td>43750</td>\n      <td>32500</td>\n      <td>61224</td>\n      <td>3643</td>\n      <td>20</td>\n      <td>...</td>\n      <td>3.74</td>\n      <td>10.31</td>\n      <td>0.23</td>\n      <td>14.44</td>\n      <td>10.77</td>\n      <td>22.19</td>\n      <td>15.48</td>\n      <td>28030</td>\n      <td>2000</td>\n      <td>POLYGON ((-70.97459559012734 42.86775028124355...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 192 columns</p>\n</div>",
      "text/plain": "         geoid  median_contract_rent  median_home_value  \\\n0  25009266400                   662             172400   \n1  25009267102                   653             169200   \n2  25009266200                   662             163200   \n3  25009267101                   624             179200   \n4  25009266100                   569             215200   \n\n   median_household_income  median_income_asianhh  median_income_blackhh  \\\n0                    53314                  48750                  22500   \n1                    50739                  46250                      0   \n2                    49315                  90457                 101277   \n3                    45625                      0                  54545   \n4                    60677                  48750                  43750   \n\n   median_income_hispanichh  median_income_whitehh  n_age_5_older  \\\n0                         0                  53808           3065   \n1                         0                  51264           4311   \n2                     26250                  48150           5131   \n3                     38750                  44750           3011   \n4                     32500                  61224           3643   \n\n   n_asian_age_distribution  ...  p_vacant_housing_units  p_veterans  \\\n0                         7  ...                    3.74        9.84   \n1                        23  ...                    3.22       13.42   \n2                        37  ...                    3.30        9.10   \n3                        11  ...                   42.08        9.96   \n4                        20  ...                    3.74       10.31   \n\n   p_vietnamese_persons  p_white_over_60  p_white_over_65  p_white_under_15  \\\n0                  0.03            14.59            11.04             21.40   \n1                  0.06            13.77             9.67             21.02   \n2                  0.07            13.44            10.63             22.38   \n3                  0.00            19.89            14.78             16.97   \n4                  0.23            14.44            10.77             22.19   \n\n   p_widowed_divorced  per_capita_income  year  \\\n0               17.61              24288  2000   \n1               18.91              20946  2000   \n2               20.74              21817  2000   \n3               27.61              22578  2000   \n4               15.48              28030  2000   \n\n                                            geometry  \n0  POLYGON ((-70.91489900000001 42.886589, -70.90...  \n1  POLYGON ((-70.91489900000001 42.886589, -70.91...  \n2  POLYGON ((-70.93079899999999 42.884589, -70.92...  \n3  POLYGON ((-70.8246893731782 42.87092164133018,...  \n4  POLYGON ((-70.97459559012734 42.86775028124355...  \n\n[5 rows x 192 columns]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.tracts_2000().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x128b5bdd8>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.states().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "datasets.tracts_1990()[datasets.tracts_1990().geoid.str.startswith('11')].dropna(subset=['median_household_income']).plot(column='median_household_income', cmap='YlOrBr', k=6, scheme='quantiles', ax=axs[0])\n",
    "axs[0].set_title(1990)\n",
    "axs[0].axis('off')\n",
    "\n",
    "datasets.tracts_2000()[datasets.tracts_2000().geoid.str.startswith('11')].dropna(subset=['median_household_income']).plot(column='median_household_income', cmap='YlOrBr', k=6, scheme='quantiles', ax=axs[1])\n",
    "axs[1].set_title(2000)\n",
    "axs[1].axis('off')\n",
    "\n",
    "datasets.tracts_2010()[datasets.tracts_2010().geoid.str.startswith('11')].dropna(subset=['median_household_income']).plot(column='median_household_income', cmap='YlOrBr', k=6, scheme='quantiles', ax=axs[2])\n",
    "axs[2].set_title(2010)\n",
    "axs[2].axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, you save these data locally for better performance using `geosnap.io.store_census`, which will download two quilt packages totaling just over 400mb (which is an exceedingly small file size, when you consider how much data are packed into those files). Once data are stored locally, you won't need this function again unless you want to update your local package to the most recent version on quilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geosnap.io import store_census\n",
    "store_census()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using geosnap's built-in data, researchers can get a jumpstart on neighborhood analysis with US tract data, but census tracts are not without their drawbacks. Many of geosnap's analytics require that neighborhood units remain consistent and stable in a study area over time (how can you analyze neighborhood change if your neighborhoods are different in each time period?), but with each new decennial census, tracts are redrawn according to population fluctuations. Geosnap offers two methods for dealing with this challenge.\n",
    "\n",
    "First, geosnap can create its own set of stable longitudinal units of analysis and convert raw census or other data into those units. Its `harmonize` module provides tools for researchers to define a set of geographic units and interpolate data into those units using moden spatial statistical methods. This is a good option for researchers who are interested in the ways that different interpolation methods can affect their analyses or those who want to use state-of-the-art methods to create longitudinal datasets that are more accurate than those provided by existing databases.\n",
    "\n",
    "Second, geosnap can simply leverage existing data that has already been standardized into a set of consistent units. The `io` module provides tools for reading and storing existing longitudinal databases that, once ingested, will be available in the data store and can be queried and analyzed repeatedly. This is a good option for researchers who want to get started modeling neighborhood characteristics right away and are less interested in exploring how error propagates through spatial interpolation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data from External Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quickest way to get started with geosnap is by importing pre-harmonized census data from either the [Longitudinal Tract Database\n",
    "(LTDB)](https://s4.ad.brown.edu/projects/diversity/Researcher/LTDB.htm) created by researchers from Brown University or the [Neighborhood Change Database](http://www.geolytics.com/USCensus,Neighborhood-Change-Database-1970-2000,Products.asp) created by the consulting company Geolytics. While licensing restrictions prevent either of these databases from being distributed inside geosnap, LTDB is nonetheless *free*. As such, we recommended importing LTDB data before getting started with geosnap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitudinal Tract Database (LTDB)\n",
    "\n",
    "The [Longitudinal Tract Database\n",
    "(LTDB)](https://s4.ad.brown.edu/projects/diversity/Researcher/LTDB.htm) is a\n",
    "freely available dataset developed by researchers at Brown University that\n",
    "provides 1970-2010 census data harmonized to 2010 boundaries.\n",
    "\n",
    "To store LTDB data and make it available to geosnap, proceed with the following:\n",
    "\n",
    "1. Download the raw data from the LTDB [downloads\n",
    "  page](https://s4.ad.brown.edu/projects/diversity/Researcher/LTBDDload/Default.aspx).\n",
    "  Note that to construct the entire database you will need two archives: one\n",
    "  containing the sample variables, and another containing the \"full count\"\n",
    "  variables.\n",
    "    - Use the dropdown menu called **select file type** and choose \"full\"; in\n",
    "      the dropdown called **select a year**, choose \"All Years\"\n",
    "    - Click the button \"Download Standard Data Files\"\n",
    "    - Repeat the process, this time selecting \"sample\" in the **select file\n",
    "      type** menu and \"All years\" in the **select a year** dropdown\n",
    "2. Note the location of the two zip archives you downloaded. By default they are called \n",
    "    - `LTDB_Std_All_Sample.zip` and\n",
    "    - `LTDB_Std_All_fullcount.zip`\n",
    "\n",
    "3. Start ipython/jupyter, import geosnap, and call the `store_ltdb` function with the paths of the two zip archives you downloaded from the LTDB project page:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geosnap.io import store_ltdb\n",
    "\n",
    "# if the archives were in my downloads folder, the paths might be something like this\n",
    "sample = \"/Users/knaaptime/Downloads/LTDB_Std_All_Sample.zip\"\n",
    "full = \"/Users/knaaptime/Downloads/LTDB_Std_All_fullcount.zip\"\n",
    "\n",
    "# uncomment to run\n",
    "#store_ltdb(sample=sample, fullcount=full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That function will extract the necessary data from the archives, calculate additional variables using formulas from the codebook, create a new local quilt package for storing the data, and register the database with the `datasets`. After the function has run, you will be able to access the LTDB data as a long-form geodataframe by calling the `ltdb` attribute from the data store. As with the `store_census` function above, this only needs to be run a single time to save the data as a local quit package and register it with geosnap. You won't neeed to store the data again unless there's an update to the variable formulas in the codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets.ltdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolytics Neighborhood Change Database\n",
    "\n",
    "The Neighborhood Change Database (ncdb) is a commercial database created by Geolytics and the Urban Institute. Like LTDB, it provides census data harmonized to 2010 tracts. NCDB data must be purchased from Geolytics prior to use. If you have a license, you can import NCDB into geosnap with the following:\n",
    "\n",
    "1. Open the Geolytics application\n",
    "2. Choose \"New Request\":   \n",
    "![Choose \"New Request\"](https://github.com/spatialucr/geosnap/blob/master/docs/figs/geolytics_interface1.PNG?raw=true)\n",
    "3. Select CSV or DBF\n",
    "4. Make the following Selections:\n",
    "    - **year**: all years in 2010 boundaries\n",
    "    - **area**: all census tracts in the entire united states\n",
    "    - **counts**: [right click] Check All Sibling Nodes\n",
    "\n",
    "![](https://github.com/spatialucr/geosnap/blob/master/docs/figs/geolytics_interface2.PNG?raw=true)\n",
    "\n",
    "5. Click `Run Report`\n",
    "\n",
    "6. Note the name and location of the CSV you created\n",
    "\n",
    "7. Start ipython/jupyter, import geosnap, and call the `store_ncdb` function with the path of the CSV:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geosnap.io import store_ncdb\n",
    "\n",
    "ncdb_path = \"~/Downloads/ncdb.csv\"\n",
    "\n",
    "# note this will raise several warnings since NCDB does not contain all the underlying data necessary to calculate all the variables in the codebook\n",
    "\n",
    "# uncomment to run\n",
    "# store_ncdb(ncdb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with above, you can access the geolytics data through the `ncdb` attribute of the `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets.ncdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geosnap]",
   "language": "python",
   "name": "conda-env-geosnap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}